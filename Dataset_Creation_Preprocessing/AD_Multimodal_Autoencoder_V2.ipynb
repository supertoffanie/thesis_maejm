{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "014f02fa-7310-4e09-acbb-104d9fa4a6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ENHANCED MCI-TO-AD PREDICTION\n",
      "================================================================================\n",
      "\n",
      "Device: cuda\n",
      "\n",
      "Loading data...\n",
      "\n",
      "Loading valid patient list...\n",
      "Valid patients: 161\n",
      "Creating dataset...\n",
      "  Processed: 161 valid patients\n",
      "  Skipped (not in valid set): 221\n",
      "Total sequences: 161\n",
      "Train: 128, Val: 33\n",
      "\n",
      "Initializing model...\n",
      "Parameters: 48,509,283\n",
      "\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "\n",
      "Epoch 1/100\n",
      "  Loss: 1.9393 (Cox: 2.2815, MMSE: 0.0000)\n",
      "  Val C-index: 0.8673\n",
      "  ✓ New best: 0.8673\n",
      "\n",
      "Epoch 2/100\n",
      "  Loss: 1.8497 (Cox: 2.1761, MMSE: 0.0000)\n",
      "  Val C-index: 0.8803\n",
      "  ✓ New best: 0.8803\n",
      "\n",
      "Epoch 3/100\n",
      "  Loss: 1.6664 (Cox: 1.9605, MMSE: 0.0000)\n",
      "  Val C-index: 0.8511\n",
      "\n",
      "Epoch 4/100\n",
      "  Loss: 1.2096 (Cox: 1.4231, MMSE: 0.0000)\n",
      "  Val C-index: 0.8220\n",
      "\n",
      "Epoch 5/100\n",
      "  Loss: 1.3658 (Cox: 1.6068, MMSE: 0.0000)\n",
      "  Val C-index: 0.7282\n",
      "\n",
      "Epoch 6/100\n",
      "  Loss: 1.3349 (Cox: 1.5705, MMSE: 0.0000)\n",
      "  Val C-index: 0.7152\n",
      "\n",
      "Epoch 7/100\n",
      "  Loss: 1.3127 (Cox: 1.5443, MMSE: 0.0000)\n",
      "  Val C-index: 0.6893\n",
      "\n",
      "Epoch 8/100\n",
      "  Loss: 1.1706 (Cox: 1.3771, MMSE: 0.0000)\n",
      "  Val C-index: 0.8285\n",
      "\n",
      "Epoch 9/100\n",
      "  Loss: 1.1585 (Cox: 1.3629, MMSE: 0.0000)\n",
      "  Val C-index: 0.8835\n",
      "  ✓ New best: 0.8835\n",
      "\n",
      "Epoch 10/100\n",
      "  Loss: 1.2048 (Cox: 1.4174, MMSE: 0.0000)\n",
      "  Val C-index: 0.8123\n",
      "\n",
      "Epoch 11/100\n",
      "  Loss: 0.9873 (Cox: 1.1615, MMSE: 0.0000)\n",
      "  Val C-index: 0.8447\n",
      "\n",
      "Epoch 12/100\n",
      "  Loss: 0.9021 (Cox: 1.0613, MMSE: 0.0000)\n",
      "  Val C-index: 0.8479\n",
      "\n",
      "Epoch 13/100\n",
      "  Loss: 0.8827 (Cox: 1.0385, MMSE: 0.0000)\n",
      "  Val C-index: 0.8026\n",
      "\n",
      "Epoch 14/100\n",
      "  Loss: 0.8287 (Cox: 0.9749, MMSE: 0.0000)\n",
      "  Val C-index: 0.8091\n",
      "\n",
      "Epoch 15/100\n",
      "  Loss: 0.7220 (Cox: 0.8494, MMSE: 0.0000)\n",
      "  Val C-index: 0.8576\n",
      "\n",
      "Epoch 16/100\n",
      "  Loss: 0.7128 (Cox: 0.8386, MMSE: 0.0000)\n",
      "  Val C-index: 0.8932\n",
      "  ✓ New best: 0.8932\n",
      "\n",
      "Epoch 17/100\n",
      "  Loss: 0.7538 (Cox: 0.8868, MMSE: 0.0000)\n",
      "  Val C-index: 0.8867\n",
      "\n",
      "Epoch 18/100\n",
      "  Loss: 0.6034 (Cox: 0.7099, MMSE: 0.0000)\n",
      "  Val C-index: 0.9126\n",
      "  ✓ New best: 0.9126\n",
      "\n",
      "Epoch 19/100\n",
      "  Loss: 0.6689 (Cox: 0.7870, MMSE: 0.0000)\n",
      "  Val C-index: 0.8803\n",
      "\n",
      "Epoch 20/100\n",
      "  Loss: 0.4673 (Cox: 0.5497, MMSE: 0.0000)\n",
      "  Val C-index: 0.8900\n",
      "\n",
      "Epoch 21/100\n",
      "  Loss: 0.5342 (Cox: 0.6285, MMSE: 0.0000)\n",
      "  Val C-index: 0.8770\n",
      "\n",
      "Epoch 22/100\n",
      "  Loss: 0.5548 (Cox: 0.6527, MMSE: 0.0000)\n",
      "  Val C-index: 0.8544\n",
      "\n",
      "Epoch 23/100\n",
      "  Loss: 0.5086 (Cox: 0.5983, MMSE: 0.0000)\n",
      "  Val C-index: 0.7443\n",
      "\n",
      "Epoch 24/100\n",
      "  Loss: 0.5009 (Cox: 0.5893, MMSE: 0.0000)\n",
      "  Val C-index: 0.7735\n",
      "\n",
      "Epoch 25/100\n",
      "  Loss: 0.5225 (Cox: 0.6147, MMSE: 0.0000)\n",
      "  Val C-index: 0.7379\n",
      "\n",
      "Epoch 26/100\n",
      "  Loss: 0.5184 (Cox: 0.6099, MMSE: 0.0000)\n",
      "  Val C-index: 0.7929\n",
      "\n",
      "Epoch 27/100\n",
      "  Loss: 0.4813 (Cox: 0.5662, MMSE: 0.0000)\n",
      "  Val C-index: 0.8058\n",
      "\n",
      "Epoch 28/100\n",
      "  Loss: 0.3918 (Cox: 0.4609, MMSE: 0.0000)\n",
      "  Val C-index: 0.8706\n",
      "\n",
      "Epoch 29/100\n",
      "  Loss: 0.4224 (Cox: 0.4969, MMSE: 0.0000)\n",
      "  Val C-index: 0.8447\n",
      "\n",
      "Epoch 30/100\n",
      "  Loss: 0.5179 (Cox: 0.6093, MMSE: 0.0000)\n",
      "  Val C-index: 0.8091\n",
      "\n",
      "Epoch 31/100\n",
      "  Loss: 0.6796 (Cox: 0.7996, MMSE: 0.0000)\n",
      "  Val C-index: 0.7605\n",
      "\n",
      "Epoch 32/100\n",
      "  Loss: 0.4435 (Cox: 0.5218, MMSE: 0.0000)\n",
      "  Val C-index: 0.8155\n",
      "\n",
      "Epoch 33/100\n",
      "  Loss: 0.3638 (Cox: 0.4280, MMSE: 0.0000)\n",
      "  Val C-index: 0.8155\n",
      "\n",
      "⚠ Early stopping\n",
      "\n",
      "================================================================================\n",
      "EXPORTING\n",
      "================================================================================\n",
      "\n",
      "Loaded best model (epoch 18)\n",
      "\n",
      "✓ Exported to latent_improved_autoencoder.csv\n",
      "  Patients: 161\n",
      "  Visits: 948\n",
      "  Latent features: 128\n",
      "\n",
      "================================================================================\n",
      "✓ COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Best C-index: 0.9126\n",
      "\n",
      "Ready for JMBayes2 in R!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Improved MCI-to-AD Conversion Prediction\n",
    "Generates latent features for JMBayes2 joint modeling in R\n",
    "\n",
    "Key Enhancements:\n",
    "1. Efron's Cox loss for tied event times\n",
    "2. Future MMSE prediction (not current)\n",
    "3. Enhanced clinical features\n",
    "4. Gradient accumulation\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "    'latent_dim': 128,\n",
    "    'img_out_dim': 256,\n",
    "    'tab_out_dim': 64,\n",
    "    'lstm_hidden': 128,\n",
    "    'lstm_layers': 2,\n",
    "    'dropout': 0.3,  # Back to 0.3 (was 0.4)\n",
    "    'lr': 5e-4,      # Back to original\n",
    "    'weight_decay': 1e-4,\n",
    "    'epochs': 100,   # Back to 100\n",
    "    'batch_size': 16,\n",
    "    'accumulation_steps': 1,  # Disable gradient accumulation for now\n",
    "    'max_seq_len': 10,\n",
    "    'alpha_survival': 0.85,  # Increased focus on survival\n",
    "    'alpha_mmse': 0.15,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE DEFINITIONS\n",
    "# ============================================================================\n",
    "DEMOGRAPHIC_COLUMNS = [\n",
    "    'AGE', 'PTGENDER_encoded', 'PTEDUCAT', 'PTETHCAT_encoded', \n",
    "    'PTRACCAT_encoded', 'PTMARRY_encoded'\n",
    "]\n",
    "\n",
    "STATIC_FEATURES = [\n",
    "    'age_bl', 'PTGENDER_encoded', 'PTEDUCAT', 'PTETHCAT_encoded', \n",
    "    'PTRACCAT_encoded', 'PTMARRY_encoded'\n",
    "]\n",
    "\n",
    "TEMPORAL_FEATURES = [\n",
    "    'time_from_baseline', 'AGE', 'age_since_bl', 'mmse_slope', \n",
    "    'adas13_slope', 'dx_progression', 'cog_decline_index', \n",
    "    'visit_number', 'MMSE', 'ADAS13'\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def add_future_mmse_label(df, horizon=1.0, window=0.25):\n",
    "    \"\"\"Add MMSE score ~12 months later\"\"\"\n",
    "    future_mmse = []\n",
    "    for i, row in df.iterrows():\n",
    "        current_time = row[\"Years_bl\"]\n",
    "        target_time = current_time + horizon\n",
    "        mask = (df[\"Years_bl\"] >= target_time - window) & \\\n",
    "               (df[\"Years_bl\"] <= target_time + window)\n",
    "        candidates = df[mask]\n",
    "        if len(candidates) > 0:\n",
    "            future_mmse.append(candidates[\"MMSE\"].iloc[0])\n",
    "        else:\n",
    "            future_mmse.append(np.nan)\n",
    "    df[\"MMSE_future12\"] = future_mmse\n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Enhanced feature engineering\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Basic temporal features\n",
    "    df[\"time_from_baseline\"] = df[\"Years_bl\"] - df[\"Years_bl\"].iloc[0]\n",
    "    df[\"age_bl\"] = df[\"AGE\"].iloc[0]\n",
    "    df[\"age_since_bl\"] = df[\"AGE\"] - df[\"age_bl\"]\n",
    "    \n",
    "    # Cognitive decline rates\n",
    "    df[\"mmse_slope\"] = df[\"MMSE\"].diff() / df[\"Years_bl\"].diff()\n",
    "    df[\"adas13_slope\"] = df[\"ADAS13\"].diff() / df[\"Years_bl\"].diff()\n",
    "    \n",
    "    # Diagnosis progression\n",
    "    dx_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2, \"Dementia\": 2}\n",
    "    df[\"dx_progression\"] = df[\"DX\"].map(dx_map).diff()\n",
    "    \n",
    "    # Cognitive decline index\n",
    "    df[\"cog_decline_index\"] = df[\"ADAS13\"] - df[\"MMSE\"]\n",
    "    \n",
    "    # Visit order\n",
    "    df[\"visit_number\"] = range(len(df))\n",
    "    \n",
    "    # Clinical interaction features\n",
    "    df['age_mmse_interaction'] = df['AGE'] * (30 - df['MMSE']) / 30\n",
    "    df['education_cognitive_reserve'] = df['PTEDUCAT'] * df['MMSE'] / 30\n",
    "    df['rapid_decline_flag'] = (df['mmse_slope'] < -2).astype(float)\n",
    "    \n",
    "    # MMSE severity\n",
    "    mmse_bins = [0, 10, 20, 24, 30]\n",
    "    df['mmse_severity'] = pd.cut(df['MMSE'], bins=mmse_bins, \n",
    "                                  labels=[3, 2, 1, 0]).astype(float)\n",
    "    \n",
    "    # Time-weighted decline\n",
    "    df['weighted_mmse_decline'] = df['mmse_slope'] * np.exp(-0.1 * df['time_from_baseline'])\n",
    "    \n",
    "    # Cognitive variability\n",
    "    df['mmse_variability'] = df['MMSE'].rolling(window=3, min_periods=1).std()\n",
    "    \n",
    "    # ADAS13-MMSE discordance\n",
    "    df['adas_mmse_discordance'] = np.abs(\n",
    "        (df['ADAS13'] - df['ADAS13'].mean()) / (df['ADAS13'].std() + 1e-7) - \n",
    "        (df['MMSE'] - df['MMSE'].mean()) / (df['MMSE'].std() + 1e-7)\n",
    "    )\n",
    "    \n",
    "    # MMSE acceleration\n",
    "    df['mmse_acceleration'] = df['mmse_slope'].diff() / df['Years_bl'].diff()\n",
    "    \n",
    "    # Cognitive efficiency\n",
    "    df['cognitive_efficiency'] = df['MMSE'] / (df['PTEDUCAT'] + 1)\n",
    "    \n",
    "    # Age-adjusted MMSE\n",
    "    expected_decline = np.maximum(0, (df['AGE'] - 65) * 0.2)\n",
    "    df['age_adjusted_mmse'] = df['MMSE'] + expected_decline\n",
    "    \n",
    "    # Fill NaNs\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # Add future MMSE label\n",
    "    df = add_future_mmse_label(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Update feature lists\n",
    "TEMPORAL_FEATURES.extend([\n",
    "    'age_mmse_interaction', 'education_cognitive_reserve', 'rapid_decline_flag',\n",
    "    'mmse_severity', 'weighted_mmse_decline', 'mmse_variability', \n",
    "    'adas_mmse_discordance', 'mmse_acceleration', 'cognitive_efficiency',\n",
    "    'age_adjusted_mmse'\n",
    "])\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET\n",
    "# ============================================================================\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Dataset providing patient-level sequences\"\"\"\n",
    "\n",
    "    def __init__(self, manifest, valid_patients, transform=None, max_seq_len=10):\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.sequences = []\n",
    "\n",
    "        manifest = manifest.copy()\n",
    "        manifest[\"path\"] = manifest[\"path\"].str.replace(\"\\\\\", \"/\", regex=False)\n",
    "        manifest[\"path\"] = \"./AD_Multimodal/TFN_AD/\" + manifest[\"path\"]\n",
    "\n",
    "        skipped_not_valid = 0\n",
    "        processed = 0\n",
    "\n",
    "        for ptid in manifest[\"PTID\"].unique():\n",
    "            if ptid not in valid_patients:\n",
    "                skipped_not_valid += 1\n",
    "                continue\n",
    "            try:\n",
    "                patient_rows = manifest[manifest[\"PTID\"] == ptid]\n",
    "                if len(patient_rows) == 0:\n",
    "                    continue\n",
    "\n",
    "                df = pd.read_pickle(patient_rows.iloc[0][\"path\"])\n",
    "                df = engineer_features(df)\n",
    "\n",
    "                dx_seq = df[\"DX\"].tolist()\n",
    "                if \"MCI\" not in dx_seq:\n",
    "                    continue\n",
    "\n",
    "                mci_idx = dx_seq.index(\"MCI\")\n",
    "                ad_idx = next(\n",
    "                    (i for i, x in enumerate(dx_seq[mci_idx + 1:], start=mci_idx + 1)\n",
    "                     if x in [\"AD\", \"Dementia\"]),\n",
    "                    -1\n",
    "                )\n",
    "\n",
    "                if ad_idx != -1:\n",
    "                    time_to_event = (\n",
    "                        df[\"Years_bl\"].iloc[ad_idx] -\n",
    "                        df[\"Years_bl\"].iloc[mci_idx]\n",
    "                    )\n",
    "                    event = 1\n",
    "                else:\n",
    "                    time_to_event = (\n",
    "                        df[\"Years_bl\"].iloc[-1] -\n",
    "                        df[\"Years_bl\"].iloc[mci_idx]\n",
    "                    )\n",
    "                    event = 0\n",
    "\n",
    "                imgs, tabs, times, mmse, mmse_future = [], [], [], [], []\n",
    "                valid_visits = 0\n",
    "\n",
    "                for _, visit in df.iterrows():\n",
    "                    image_path = visit[\"image_path\"].replace(\n",
    "                        \"/home/mason/ADNI_Dataset/\",\n",
    "                        \"./AD_Multimodal/ADNI_Dataset/\"\n",
    "                    )\n",
    "\n",
    "                    if not os.path.exists(image_path):\n",
    "                        continue\n",
    "\n",
    "                    img = Image.open(image_path).convert(\"RGB\")\n",
    "                    if self.transform:\n",
    "                        img = self.transform(img)\n",
    "\n",
    "                    imgs.append(img)\n",
    "                    tabs.append(\n",
    "                        visit[TEMPORAL_FEATURES + STATIC_FEATURES]\n",
    "                        .values.astype(np.float32)\n",
    "                    )\n",
    "                    times.append(visit[\"Years_bl\"])\n",
    "                    mmse.append(visit[\"MMSE\"])\n",
    "                    mmse_future.append(\n",
    "                        visit[\"MMSE_future12\"]\n",
    "                        if \"MMSE_future12\" in visit else np.nan\n",
    "                    )\n",
    "\n",
    "                    valid_visits += 1\n",
    "                    if valid_visits >= self.max_seq_len:\n",
    "                        break\n",
    "\n",
    "                if valid_visits < 2:\n",
    "                    continue\n",
    "\n",
    "                pad_len = self.max_seq_len - valid_visits\n",
    "                if pad_len > 0:\n",
    "                    for _ in range(pad_len):\n",
    "                        imgs.append(torch.zeros_like(imgs[-1]))\n",
    "                        tabs.append(np.zeros_like(tabs[-1]))\n",
    "                        times.append(times[-1])\n",
    "                        mmse.append(np.nan)\n",
    "                        mmse_future.append(np.nan)\n",
    "\n",
    "                self.sequences.append({\n",
    "                    \"ptid\": ptid,\n",
    "                    \"imgs\": torch.stack(imgs),\n",
    "                    \"tabs\": np.array(tabs, dtype=np.float32),\n",
    "                    \"times\": np.array(times, dtype=np.float32),\n",
    "                    \"mmse\": np.array(mmse, dtype=np.float32),\n",
    "                    \"mmse_future\": np.array(mmse_future, dtype=np.float32),\n",
    "                    \"seq_len\": valid_visits,\n",
    "                    \"time_to_event\": float(time_to_event),\n",
    "                    \"event\": int(event)\n",
    "                })\n",
    "\n",
    "                processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Skipped patient {ptid}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"  Processed: {processed} valid patients\")\n",
    "        print(f\"  Skipped (not in valid set): {skipped_not_valid}\")\n",
    "\n",
    "        # all_tabs = np.vstack([seq[\"tabs\"] for seq in self.sequences])\n",
    "        # self.scaler = StandardScaler()\n",
    "        # self.scaler.fit(all_tabs)\n",
    "\n",
    "        # CRITICAL R FIX: Standardize tabular features for numerical stability\n",
    "        if len(self.sequences) > 0:\n",
    "            all_tabs = np.vstack([seq['tabs'] for seq in self.sequences])\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(all_tabs)\n",
    "            \n",
    "            for seq in self.sequences:\n",
    "                seq['tabs'] = self.scaler.transform(seq['tabs']).astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        return (\n",
    "            seq['imgs'],\n",
    "            seq['tabs'],\n",
    "            seq['times'],\n",
    "            seq['mmse'],\n",
    "            seq['mmse_future'],\n",
    "            seq['seq_len'],\n",
    "            seq['time_to_event'],\n",
    "            seq['event'],\n",
    "            seq['ptid']\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class TensorFusion(nn.Module):\n",
    "    \"\"\"Tensor fusion layer\"\"\"\n",
    "    def __init__(self, v_dim, d_dim, t_dim, proj_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.v_dim = v_dim\n",
    "        self.d_dim = d_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.output_dim = (v_dim + 1) * (d_dim + 1) * (t_dim + 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if proj_dim:\n",
    "            self.proj = nn.Linear(self.output_dim, proj_dim)\n",
    "        else:\n",
    "            self.proj = None\n",
    "    \n",
    "    def forward(self, v, d, t):\n",
    "        batch_size = v.shape[0]\n",
    "        \n",
    "        v_1 = torch.cat([v, torch.ones(batch_size, 1, device=v.device)], dim=1)\n",
    "        d_1 = torch.cat([d, torch.ones(batch_size, 1, device=d.device)], dim=1)\n",
    "        t_1 = torch.cat([t, torch.ones(batch_size, 1, device=t.device)], dim=1)\n",
    "        \n",
    "        fusion = torch.einsum('bi,bj,bk->bijk', v_1, d_1, t_1)\n",
    "        fusion = fusion.view(batch_size, -1)\n",
    "        \n",
    "        fusion = self.dropout(fusion)\n",
    "        \n",
    "        if self.proj:\n",
    "            fusion = self.proj(fusion)\n",
    "        \n",
    "        return fusion\n",
    "\n",
    "class AttentionImageEncoder(nn.Module):\n",
    "    \"\"\"Image encoder with spatial attention\"\"\"\n",
    "    def __init__(self, out_dim=256):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(pretrained=True)\n",
    "        \n",
    "        for param in list(base.parameters())[:-20]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.features = nn.Sequential(*list(base.children())[:-2])\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.proj = nn.Linear(512, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feats = self.features(x)\n",
    "        attn = self.attention(feats)\n",
    "        feats = feats * attn\n",
    "        pooled = self.global_pool(feats).view(x.size(0), -1)\n",
    "        return self.proj(pooled)\n",
    "\n",
    "class SupervisedTemporalFusionAutoencoder(nn.Module):\n",
    "    \"\"\"Multi-modal temporal model\"\"\"\n",
    "    def __init__(self, tab_dim, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.img_encoder = AttentionImageEncoder(out_dim=config['img_out_dim'])\n",
    "        \n",
    "        self.tab_encoder = nn.Sequential(\n",
    "            nn.Linear(tab_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(128, config['tab_out_dim']),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.time_encoder = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fusion = TensorFusion(\n",
    "            v_dim=config['img_out_dim'],\n",
    "            d_dim=config['tab_out_dim'],\n",
    "            t_dim=16,\n",
    "            dropout=config['dropout']\n",
    "        )\n",
    "        \n",
    "        fusion_dim = (config['img_out_dim'] + 1) * (config['tab_out_dim'] + 1) * 17\n",
    "        self.fusion_proj = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, config['latent_dim']),\n",
    "            nn.BatchNorm1d(config['latent_dim']),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config['latent_dim'],\n",
    "            hidden_size=config['lstm_hidden'],\n",
    "            num_layers=config['lstm_layers'],\n",
    "            batch_first=True,\n",
    "            dropout=config['dropout'] if config['lstm_layers'] > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        self.temporal_proj = nn.Sequential(\n",
    "            nn.Linear(config['lstm_hidden'] * 2, config['latent_dim']),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.survival_head = nn.Sequential(\n",
    "            nn.Linear(config['latent_dim'], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "        self.mmse_head = nn.Sequential(\n",
    "            nn.Linear(config['latent_dim'], 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def encode_visit(self, img, tab, time):\n",
    "        \"\"\"Encode a single visit\"\"\"\n",
    "        v = self.img_encoder(img)\n",
    "        d = self.tab_encoder(tab)\n",
    "        t = self.time_encoder(time.unsqueeze(1))\n",
    "        \n",
    "        z = self.fusion(v, d, t)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fusion_proj(z)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def forward(self, img_seq, tab_seq, time_seq, seq_lengths):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        batch_size, seq_len = img_seq.shape[:2]\n",
    "        \n",
    "        z_list = []\n",
    "        for t in range(seq_len):\n",
    "            img_t = img_seq[:, t]\n",
    "            tab_t = tab_seq[:, t]\n",
    "            time_t = time_seq[:, t]\n",
    "            \n",
    "            z_t = self.encode_visit(img_t, tab_t, time_t)\n",
    "            z_list.append(z_t)\n",
    "        \n",
    "        z_seq = torch.stack(z_list, dim=1)\n",
    "        \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            z_seq, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        lstm_out, (h_n, c_n) = self.lstm(packed)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        h_forward = h_n[-2]\n",
    "        h_backward = h_n[-1]\n",
    "        h_final = torch.cat([h_forward, h_backward], dim=1)\n",
    "        \n",
    "        z_final = self.temporal_proj(h_final)\n",
    "        \n",
    "        risk_score = self.survival_head(z_final)\n",
    "        mmse_pred = self.mmse_head(z_final)\n",
    "        \n",
    "        return z_final, risk_score, mmse_pred\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def cox_partial_likelihood_loss_efron(risk_scores, times, events):\n",
    "    \"\"\"Cox loss with Efron's approximation\"\"\"\n",
    "    device = risk_scores.device\n",
    "    \n",
    "    mask = torch.isfinite(risk_scores) & torch.isfinite(times)\n",
    "    risk_scores = risk_scores[mask]\n",
    "    times = times[mask]\n",
    "    events = events[mask]\n",
    "    \n",
    "    if events.sum() == 0:\n",
    "        return torch.tensor(0.0, device=device)\n",
    "    \n",
    "    order = torch.argsort(times, descending=False)\n",
    "    risk_scores = risk_scores[order]\n",
    "    times = times[order]\n",
    "    events = events[order]\n",
    "    \n",
    "    log_risk = risk_scores.view(-1)\n",
    "    hazard = torch.exp(log_risk)\n",
    "    \n",
    "    unique_times = torch.unique(times[events == 1])\n",
    "    \n",
    "    loss = torch.tensor(0.0, device=device)\n",
    "    for t in unique_times:\n",
    "        at_risk = times >= t\n",
    "        died = (times == t) & (events == 1)\n",
    "        \n",
    "        if died.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        risk_set = hazard[at_risk].sum()\n",
    "        died_risk = log_risk[died].sum()\n",
    "        n_died = died.sum().float()\n",
    "        \n",
    "        tied_risk = hazard[died].sum()\n",
    "        \n",
    "        for j in range(int(n_died)):\n",
    "            risk_set_j = risk_set - (j / n_died) * tied_risk\n",
    "            loss += torch.log(risk_set_j + 1e-7)\n",
    "        \n",
    "        loss -= died_risk\n",
    "    \n",
    "    return loss / (events.sum() + 1e-7)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch(model, loader, optimizer, config, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_cox = 0\n",
    "    total_mmse = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        imgs, tabs, times, mmse, mmse_future, seq_lens, t_event, event, _ = batch\n",
    "        \n",
    "        imgs = imgs.to(device)\n",
    "        tabs = torch.FloatTensor(tabs).to(device)\n",
    "        times = torch.FloatTensor(times).to(device)\n",
    "        mmse_future_vals = torch.FloatTensor(mmse_future).to(device)\n",
    "        seq_lens = torch.LongTensor(seq_lens)\n",
    "        t_event = t_event.float().to(device)\n",
    "        event = event.float().to(device)\n",
    "        \n",
    "        # Get FUTURE MMSE targets (not current!)\n",
    "        mmse_targets = []\n",
    "        for i, slen in enumerate(seq_lens):\n",
    "            future_val = mmse_future_vals[i, slen-1]\n",
    "            mmse_targets.append(future_val if not torch.isnan(future_val) else torch.tensor(float('nan')))\n",
    "        mmse_targets = torch.stack(mmse_targets).to(device)\n",
    "        \n",
    "        # Forward\n",
    "        z_final, risk_scores, mmse_pred = model(imgs, tabs, times, seq_lens)\n",
    "        \n",
    "        # Cox loss (Efron)\n",
    "        loss_cox = cox_partial_likelihood_loss_efron(\n",
    "            risk_scores.squeeze(), t_event, event\n",
    "        )\n",
    "        \n",
    "        # MMSE loss (predict FUTURE, not current)\n",
    "        valid_mmse_mask = ~torch.isnan(mmse_targets)\n",
    "        if valid_mmse_mask.any():\n",
    "            loss_mmse = F.mse_loss(\n",
    "                mmse_pred.squeeze()[valid_mmse_mask],\n",
    "                mmse_targets[valid_mmse_mask]\n",
    "            )\n",
    "        else:\n",
    "            loss_mmse = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Combined\n",
    "        loss = (config['alpha_survival'] * loss_cox + \n",
    "                config['alpha_mmse'] * loss_mmse)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_cox += loss_cox.item()\n",
    "        total_mmse += loss_mmse.item() if valid_mmse_mask.any() else 0\n",
    "    \n",
    "    n_batches = len(loader)\n",
    "    return {\n",
    "        'total': total_loss / n_batches,\n",
    "        'cox': total_cox / n_batches,\n",
    "        'mmse': total_mmse / n_batches,\n",
    "    }\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    \"\"\"Validate and compute C-index\"\"\"\n",
    "    model.eval()\n",
    "    all_risks = []\n",
    "    all_times = []\n",
    "    all_events = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            imgs, tabs, times, mmse, mmse_future, seq_lens, t_event, event, _ = batch\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            tabs = torch.FloatTensor(tabs).to(device)\n",
    "            times = torch.FloatTensor(times).to(device)\n",
    "            seq_lens = torch.LongTensor(seq_lens)\n",
    "            \n",
    "            _, risk_scores, _ = model(imgs, tabs, times, seq_lens)\n",
    "            \n",
    "            all_risks.extend(risk_scores.cpu().numpy().flatten())\n",
    "            all_times.extend(t_event.numpy())\n",
    "            all_events.extend(event.numpy())\n",
    "    \n",
    "    all_events = np.array(all_events).astype(bool)\n",
    "    all_times = np.array(all_times)\n",
    "    all_risks = np.array(all_risks)\n",
    "    \n",
    "    c_index = concordance_index(all_times, -all_risks, all_events)\n",
    "    \n",
    "    return c_index, all_risks, all_times, all_events\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT\n",
    "# ============================================================================\n",
    "\n",
    "def export_latent_features(model, loader, device, output_path):\n",
    "    \"\"\"Export for JMBayes2\"\"\"\n",
    "    model.eval()\n",
    "    rows = []\n",
    "\n",
    "    BASELINE_FEATURES = ['AGE', 'PTGENDER', 'PTEDUCAT', 'ADAS13']\n",
    "    tab_feature_names = TEMPORAL_FEATURES + STATIC_FEATURES\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            imgs, tabs, times, mmse, mmse_future, seq_lens, t_event, event, ptids = batch\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            tabs = tabs.to(device)\n",
    "            times = times.to(device)\n",
    "            seq_lens = seq_lens.cpu().long()\n",
    "\n",
    "            for i in range(len(ptids)):\n",
    "                slen = seq_lens[i].item()\n",
    "\n",
    "                for t in range(slen):\n",
    "                    img_t = imgs[i:i+1, t]\n",
    "                    tab_t = tabs[i:i+1, t]\n",
    "                    time_t = times[i:i+1, t]\n",
    "\n",
    "                    z_visit = model.encode_visit(img_t, tab_t, time_t)\n",
    "\n",
    "                    tab_vals_unscaled = loader.dataset.scaler.inverse_transform(\n",
    "                        tabs[i, t].cpu().numpy().reshape(1, -1)\n",
    "                    )[0]\n",
    "\n",
    "                    row = {\n",
    "                        \"PTID\": ptids[i],\n",
    "                        \"Years_bl\": float(times[i, t].cpu()),\n",
    "                        \"MMSE\": float(mmse[i, t]),\n",
    "                        \"time_to_event\": float(t_event[i]),\n",
    "                        \"event\": int(event[i]),\n",
    "                    }\n",
    "\n",
    "                    for feat in BASELINE_FEATURES:\n",
    "                        feat_encoded = feat + '_encoded' if feat == 'PTGENDER' else feat\n",
    "                        if feat_encoded in tab_feature_names:\n",
    "                            idx = tab_feature_names.index(feat_encoded)\n",
    "                            row[feat] = float(tab_vals_unscaled[idx])\n",
    "\n",
    "                    z_vals = z_visit[0].cpu().numpy()\n",
    "                    for k, val in enumerate(z_vals):\n",
    "                        row[f\"z_{k}\"] = float(val)\n",
    "\n",
    "                    rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([\"PTID\", \"Years_bl\"])\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"\\n✓ Exported to {output_path}\")\n",
    "    print(f\"  Patients: {df['PTID'].nunique()}\")\n",
    "    print(f\"  Visits: {len(df)}\")\n",
    "    print(f\"  Latent features: {len([c for c in df.columns if c.startswith('z_')])}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ENHANCED MCI-TO-AD PREDICTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    device = CONFIG['device']\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading data...\")\n",
    "    manifest = pd.read_csv(\"./AD_Multimodal/TFN_AD/AD_Patient_Manifest.csv\")\n",
    "\n",
    "    # Load valid patients\n",
    "    print(\"\\nLoading valid patient list...\")\n",
    "    with open('VALID_PATIENTS.pkl', 'rb') as f:\n",
    "        VALID_PATIENTS = pickle.load(f)\n",
    "    print(f\"Valid patients: {len(VALID_PATIENTS)}\")\n",
    "    \n",
    "    # Transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Dataset\n",
    "    print(\"Creating dataset...\")\n",
    "    dataset = SequenceDataset(manifest, VALID_PATIENTS, transform, max_seq_len=CONFIG['max_seq_len'])\n",
    "    print(f\"Total sequences: {len(dataset)}\")\n",
    "    \n",
    "    # Split\n",
    "    n_train = int(0.8 * len(dataset))\n",
    "    n_val = len(dataset) - n_train\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [n_train, n_val],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                             shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
    "                           shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Train: {n_train}, Val: {n_val}\")\n",
    "    \n",
    "    # Model\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    tab_dim = sample_batch[1].shape[2]\n",
    "    \n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = SupervisedTemporalFusionAutoencoder(tab_dim, CONFIG).to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Parameters: {total_params:,}\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=CONFIG['lr'], \n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    best_c_index = 0\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        train_losses = train_epoch(model, train_loader, optimizer, CONFIG, device)\n",
    "        val_c_index, _, _, _ = validate(model, val_loader, device)\n",
    "        \n",
    "        scheduler.step(val_c_index)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "        print(f\"  Loss: {train_losses['total']:.4f} \"\n",
    "              f\"(Cox: {train_losses['cox']:.4f}, MMSE: {train_losses['mmse']:.4f})\")\n",
    "        print(f\"  Val C-index: {val_c_index:.4f}\")\n",
    "        \n",
    "        if val_c_index > best_c_index:\n",
    "            best_c_index = val_c_index\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'c_index': best_c_index,\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"  ✓ New best: {best_c_index:.4f}\")\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        \n",
    "        if patience >= 15:\n",
    "            print(f\"\\n⚠ Early stopping\")\n",
    "            break\n",
    "    \n",
    "    # Export\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPORTING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # FIX: Use weights_only=False for backward compatibility\n",
    "    checkpoint = torch.load('best_model.pth', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"\\nLoaded best model (epoch {checkpoint['epoch']+1})\")\n",
    "    \n",
    "    full_loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], \n",
    "                            shuffle=False, num_workers=0)\n",
    "    \n",
    "    latent_df = export_latent_features(\n",
    "        model, full_loader, device, \n",
    "        \"latent_improved_autoencoder.csv\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nBest C-index: {best_c_index:.4f}\")\n",
    "    print(\"\\nReady for JMBayes2 in R!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48b237-59da-4761-9ddf-7a532a23f855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6d906-e2f1-465d-b6d2-3a0081e8fe0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
