{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65445c8-ffeb-443b-bdf7-5010c608b80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BENCHMARK 3: TABULAR-ONLY DEEP NETWORK\n",
      "================================================================================\n",
      "\n",
      "Device: cuda\n",
      "\n",
      "Loading valid patient list...\n",
      "Valid patients: 161\n",
      "  Processed: 161 valid patients\n",
      "  Skipped (not in valid set): 221\n",
      "Total sequences: 161\n",
      "Tabular features: 23\n",
      "\n",
      "Training...\n",
      "Epoch 1/100 - Loss: 2.2746, C-index: 0.8350\n",
      "Epoch 2/100 - Loss: 2.2125, C-index: 0.8188\n",
      "Epoch 3/100 - Loss: 1.9936, C-index: 0.8252\n",
      "Epoch 4/100 - Loss: 1.8367, C-index: 0.8285\n",
      "Epoch 5/100 - Loss: 1.8272, C-index: 0.8414\n",
      "Epoch 6/100 - Loss: 1.7868, C-index: 0.8414\n",
      "Epoch 7/100 - Loss: 1.7790, C-index: 0.8447\n",
      "Epoch 8/100 - Loss: 1.7369, C-index: 0.8414\n",
      "Epoch 9/100 - Loss: 1.7838, C-index: 0.8414\n",
      "Epoch 10/100 - Loss: 1.7139, C-index: 0.8350\n",
      "Epoch 11/100 - Loss: 1.6903, C-index: 0.8350\n",
      "Epoch 12/100 - Loss: 1.6914, C-index: 0.8252\n",
      "Epoch 13/100 - Loss: 1.5982, C-index: 0.8220\n",
      "Epoch 14/100 - Loss: 1.6423, C-index: 0.8285\n",
      "Epoch 15/100 - Loss: 1.7197, C-index: 0.8317\n",
      "Epoch 16/100 - Loss: 1.5417, C-index: 0.8188\n",
      "Epoch 17/100 - Loss: 1.6740, C-index: 0.8220\n",
      "Epoch 18/100 - Loss: 1.6874, C-index: 0.8155\n",
      "Epoch 19/100 - Loss: 1.6328, C-index: 0.8188\n",
      "Epoch 20/100 - Loss: 1.5952, C-index: 0.8220\n",
      "Epoch 21/100 - Loss: 1.6337, C-index: 0.8220\n",
      "Epoch 22/100 - Loss: 1.6426, C-index: 0.8220\n",
      "Early stopping\n",
      "\n",
      "✓ Exported to tabular_only_features.csv\n",
      "  Patients: 161, Features: 128\n",
      "\n",
      "================================================================================\n",
      "✓ BEST C-INDEX: 0.8447\n",
      "⚠️  This uses the SAME patient cohort as all other benchmarks\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Benchmark 3: Tabular-Only Deep Neural Network\n",
    "Tests whether deep learning on clinical features alone helps\n",
    "\n",
    "FIXED VERSION:\n",
    "- Uses VALID_PATIENTS.pkl for consistent patient cohort\n",
    "- Survival time measured from MCI diagnosis (consistent)\n",
    "- Same feature engineering as thesis\n",
    "- Only tabular features (NO images)\n",
    "\n",
    "Architecture:\n",
    "- Deep feedforward network on tabular data\n",
    "- LSTM for temporal modeling\n",
    "- Shows value of adding imaging modality\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from lifelines.utils import concordance_index\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CONFIG = {\n",
    "    'tab_hidden': 256,\n",
    "    'lstm_hidden': 128,\n",
    "    'lstm_layers': 2,\n",
    "    'dropout': 0.3,\n",
    "    'lr': 5e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 16,\n",
    "    'max_seq_len': 10,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING (Same as thesis)\n",
    "# ============================================================================\n",
    "\n",
    "STATIC_FEATURES = [\n",
    "    'age_bl', 'PTGENDER_encoded', 'PTEDUCAT', 'PTETHCAT_encoded', \n",
    "    'PTRACCAT_encoded', 'PTMARRY_encoded'\n",
    "]\n",
    "\n",
    "TEMPORAL_FEATURES = [\n",
    "    'time_from_baseline', 'AGE', 'age_since_bl', 'mmse_slope', \n",
    "    'adas13_slope', 'dx_progression', 'cog_decline_index', \n",
    "    'visit_number', 'MMSE', 'ADAS13'\n",
    "]\n",
    "\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df[\"time_from_baseline\"] = df[\"Years_bl\"] - df[\"Years_bl\"].iloc[0]\n",
    "    df[\"age_bl\"] = df[\"AGE\"].iloc[0]\n",
    "    df[\"age_since_bl\"] = df[\"AGE\"] - df[\"age_bl\"]\n",
    "    \n",
    "    df[\"mmse_slope\"] = df[\"MMSE\"].diff() / df[\"Years_bl\"].diff()\n",
    "    df[\"adas13_slope\"] = df[\"ADAS13\"].diff() / df[\"Years_bl\"].diff()\n",
    "    \n",
    "    dx_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2, \"Dementia\": 2}\n",
    "    df[\"dx_progression\"] = df[\"DX\"].map(dx_map).diff()\n",
    "    \n",
    "    df[\"cog_decline_index\"] = df[\"ADAS13\"] - df[\"MMSE\"]\n",
    "    df[\"visit_number\"] = range(len(df))\n",
    "    \n",
    "    df['age_mmse_interaction'] = df['AGE'] * (30 - df['MMSE']) / 30\n",
    "    df['education_cognitive_reserve'] = df['PTEDUCAT'] * df['MMSE'] / 30\n",
    "    df['rapid_decline_flag'] = (df['mmse_slope'] < -2).astype(float)\n",
    "    \n",
    "    mmse_bins = [0, 20, 24, 30]\n",
    "    df['mmse_severity'] = pd.cut(df['MMSE'], bins=mmse_bins, labels=[2, 1, 0]).astype(float)\n",
    "    \n",
    "    df['weighted_mmse_decline'] = df['mmse_slope'] * np.exp(-0.1 * df['time_from_baseline'])\n",
    "    df['mmse_variability'] = df['MMSE'].rolling(window=3, min_periods=1).std()\n",
    "    \n",
    "    df['adas_mmse_discordance'] = np.abs(\n",
    "        (df['ADAS13'] - df['ADAS13'].mean()) / (df['ADAS13'].std() + 1e-7) - \n",
    "        (df['MMSE'] - df['MMSE'].mean()) / (df['MMSE'].std() + 1e-7)\n",
    "    )\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "TEMPORAL_FEATURES.extend([\n",
    "    'age_mmse_interaction', 'education_cognitive_reserve', 'rapid_decline_flag',\n",
    "    'mmse_severity', 'weighted_mmse_decline', 'mmse_variability', 'adas_mmse_discordance'\n",
    "])\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET\n",
    "# ============================================================================\n",
    "\n",
    "class TabularOnlyDataset(Dataset):\n",
    "    def __init__(self, manifest, valid_patients, max_seq_len=10):\n",
    "        self.sequences = []\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        manifest[\"path\"] = manifest[\"path\"].str.replace(\"\\\\\", \"/\", regex=False)\n",
    "        manifest[\"path\"] = \"./AD_Multimodal/TFN_AD/\" + manifest[\"path\"]\n",
    "        \n",
    "        processed = 0\n",
    "        skipped_not_valid = 0\n",
    "        \n",
    "        for ptid in manifest['PTID'].unique():\n",
    "            # CRITICAL: Only process valid patients\n",
    "            if ptid not in valid_patients:\n",
    "                skipped_not_valid += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                patient_rows = manifest[manifest['PTID'] == ptid]\n",
    "                if len(patient_rows) == 0:\n",
    "                    continue\n",
    "                \n",
    "                df = pd.read_pickle(patient_rows.iloc[0][\"path\"])\n",
    "                df = engineer_features(df)\n",
    "                \n",
    "                dx_seq = df[\"DX\"].tolist()\n",
    "                if \"MCI\" not in dx_seq:\n",
    "                    continue\n",
    "                \n",
    "                # FIXED: Time from MCI diagnosis\n",
    "                mci_idx = dx_seq.index(\"MCI\")\n",
    "                ad_idx = next((i for i, x in enumerate(dx_seq[mci_idx+1:], start=mci_idx+1) \n",
    "                              if x in [\"AD\", \"Dementia\"]), -1)\n",
    "                \n",
    "                if ad_idx != -1:\n",
    "                    time_to_event = df[\"Years_bl\"].iloc[ad_idx] - df[\"Years_bl\"].iloc[mci_idx]\n",
    "                    event = 1\n",
    "                else:\n",
    "                    time_to_event = df[\"Years_bl\"].iloc[-1] - df[\"Years_bl\"].iloc[mci_idx]\n",
    "                    event = 0\n",
    "                \n",
    "                tabs, times = [], []\n",
    "                valid_visits = 0\n",
    "                \n",
    "                for _, visit in df.iterrows():\n",
    "                    tabs.append(visit[TEMPORAL_FEATURES + STATIC_FEATURES].values.astype(np.float32))\n",
    "                    times.append(visit[\"Years_bl\"])\n",
    "                    valid_visits += 1\n",
    "                    \n",
    "                    if valid_visits >= max_seq_len:\n",
    "                        break\n",
    "                \n",
    "                if valid_visits < 2:\n",
    "                    continue\n",
    "                \n",
    "                pad_len = max_seq_len - len(tabs)\n",
    "                if pad_len > 0:\n",
    "                    for _ in range(pad_len):\n",
    "                        tabs.append(np.zeros_like(tabs[-1]))\n",
    "                        times.append(times[-1])\n",
    "                \n",
    "                self.sequences.append({\n",
    "                    'ptid': ptid,\n",
    "                    'tabs': np.array(tabs, dtype=np.float32),\n",
    "                    'times': np.array(times, dtype=np.float32),\n",
    "                    'seq_len': valid_visits,\n",
    "                    'time_to_event': time_to_event,\n",
    "                    'event': event\n",
    "                })\n",
    "                \n",
    "                processed += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"  Processed: {processed} valid patients\")\n",
    "        print(f\"  Skipped (not in valid set): {skipped_not_valid}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        return (\n",
    "            seq['tabs'], seq['times'], seq['seq_len'],\n",
    "            seq['time_to_event'], seq['event'], seq['ptid']\n",
    "        )\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class TabularOnlyModel(nn.Module):\n",
    "    def __init__(self, tab_dim, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Deep tabular encoder\n",
    "        self.tab_encoder = nn.Sequential(\n",
    "            nn.Linear(tab_dim, config['tab_hidden']),\n",
    "            nn.BatchNorm1d(config['tab_hidden']),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(config['tab_hidden'], config['tab_hidden'] // 2),\n",
    "            nn.BatchNorm1d(config['tab_hidden'] // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(config['tab_hidden'] // 2, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=config['lstm_hidden'],\n",
    "            num_layers=config['lstm_layers'],\n",
    "            batch_first=True,\n",
    "            dropout=config['dropout'] if config['lstm_layers'] > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Risk prediction\n",
    "        self.risk_head = nn.Sequential(\n",
    "            nn.Linear(config['lstm_hidden'] * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def encode_tabular(self, tab):\n",
    "        return self.tab_encoder(tab)\n",
    "    \n",
    "    def forward(self, tab_seq, seq_lengths):\n",
    "        batch_size, seq_len, _ = tab_seq.shape\n",
    "        \n",
    "        # Encode each timestep\n",
    "        tab_feats = []\n",
    "        for t in range(seq_len):\n",
    "            feat = self.encode_tabular(tab_seq[:, t])\n",
    "            tab_feats.append(feat)\n",
    "        \n",
    "        tab_seq_feat = torch.stack(tab_feats, dim=1)\n",
    "        \n",
    "        # LSTM\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            tab_seq_feat, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        lstm_out, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        h_forward = h_n[-2]\n",
    "        h_backward = h_n[-1]\n",
    "        h_final = torch.cat([h_forward, h_backward], dim=1)\n",
    "        \n",
    "        risk_score = self.risk_head(h_final)\n",
    "        \n",
    "        return risk_score\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS & TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def cox_loss(risk_scores, times, events):\n",
    "    order = torch.argsort(times, descending=True)\n",
    "    risk_scores = risk_scores[order]\n",
    "    events = events[order]\n",
    "    \n",
    "    log_risk = risk_scores.view(-1)\n",
    "    log_cumsum_hazard = torch.logcumsumexp(log_risk, dim=0)\n",
    "    loss = -(log_risk - log_cumsum_hazard) * events\n",
    "    \n",
    "    return loss.sum() / (events.sum() + 1e-7)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        tabs, times, seq_lens, t_event, event, _ = batch\n",
    "        \n",
    "        tabs = torch.FloatTensor(tabs).to(device)\n",
    "        seq_lens = torch.LongTensor(seq_lens)\n",
    "        t_event = t_event.float().to(device)\n",
    "        event = event.float().to(device)\n",
    "        \n",
    "        risk_scores = model(tabs, seq_lens)\n",
    "        loss = cox_loss(risk_scores.squeeze(), t_event, event)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_risks, all_times, all_events = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            tabs, times, seq_lens, t_event, event, _ = batch\n",
    "            \n",
    "            tabs = torch.FloatTensor(tabs).to(device)\n",
    "            seq_lens = torch.LongTensor(seq_lens)\n",
    "            \n",
    "            risk_scores = model(tabs, seq_lens)\n",
    "            \n",
    "            all_risks.extend(risk_scores.cpu().numpy().flatten())\n",
    "            all_times.extend(t_event.numpy())\n",
    "            all_events.extend(event.numpy())\n",
    "    \n",
    "    all_events = np.array(all_events).astype(bool)\n",
    "    all_times = np.array(all_times)\n",
    "    all_risks = np.array(all_risks)\n",
    "    \n",
    "    c_index = concordance_index(all_times, -all_risks, all_events)\n",
    "    return c_index\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT\n",
    "# ============================================================================\n",
    "\n",
    "def export_features(model, loader, device, output_path):\n",
    "    \"\"\"Export tabular features - baseline visit only for survival modeling\"\"\"\n",
    "    model.eval()\n",
    "    rows = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            tabs, times, seq_lens, t_event, event, ptids = batch\n",
    "            \n",
    "            tabs = torch.FloatTensor(tabs).to(device)\n",
    "            seq_lens = torch.LongTensor(seq_lens)\n",
    "            \n",
    "            # Only use FIRST visit for each patient (baseline)\n",
    "            for i in range(len(ptids)):\n",
    "                tab_feat = model.encode_tabular(tabs[i:i+1, 0])\n",
    "                tab_vals = tabs[i, 0].cpu().numpy()\n",
    "                \n",
    "                row = {\n",
    "                    \"PTID\": ptids[i],\n",
    "                    \"Years_bl\": float(times[i][0]),\n",
    "                    \"time_to_event\": float(t_event[i]),\n",
    "                    \"event\": int(event[i])\n",
    "                }\n",
    "                \n",
    "                feat_vals = tab_feat[0].cpu().numpy()\n",
    "                for k in range(len(feat_vals)):\n",
    "                    row[f\"tab_feat_{k}\"] = float(feat_vals[k])\n",
    "                \n",
    "                rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows).sort_values(['PTID', 'Years_bl'])\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Exported to {output_path}\")\n",
    "    print(f\"  Patients: {df['PTID'].nunique()}, Features: {len([c for c in df.columns if c.startswith('tab_feat')])}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"BENCHMARK 3: TABULAR-ONLY DEEP NETWORK\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    device = CONFIG['device']\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    \n",
    "    # Load valid patients\n",
    "    print(\"\\nLoading valid patient list...\")\n",
    "    with open('VALID_PATIENTS.pkl', 'rb') as f:\n",
    "        VALID_PATIENTS = pickle.load(f)\n",
    "    print(f\"Valid patients: {len(VALID_PATIENTS)}\")\n",
    "    \n",
    "    manifest = pd.read_csv(\"./AD_Multimodal/TFN_AD/AD_Patient_Manifest.csv\")\n",
    "    \n",
    "    dataset = TabularOnlyDataset(manifest, VALID_PATIENTS, max_seq_len=CONFIG['max_seq_len'])\n",
    "    print(f\"Total sequences: {len(dataset)}\")\n",
    "    \n",
    "    n_train = int(0.8 * len(dataset))\n",
    "    n_val = len(dataset) - n_train\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                             shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
    "                           shuffle=False, num_workers=0)\n",
    "    \n",
    "    sample_batch = next(iter(train_loader))\n",
    "    tab_dim = sample_batch[0].shape[2]\n",
    "    print(f\"Tabular features: {tab_dim}\")\n",
    "    \n",
    "    model = TabularOnlyModel(tab_dim, CONFIG).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'], \n",
    "                                  weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining...\")\n",
    "    best_c_index = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        val_c_index = validate(model, val_loader, device)\n",
    "        \n",
    "        scheduler.step(val_c_index)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs']} - Loss: {train_loss:.4f}, C-index: {val_c_index:.4f}\")\n",
    "        \n",
    "        if val_c_index > best_c_index:\n",
    "            best_c_index = val_c_index\n",
    "            torch.save(model.state_dict(), 'tabular_only_model.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= 15:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    \n",
    "    # Export\n",
    "    model.load_state_dict(torch.load('tabular_only_model.pth'))\n",
    "    full_loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], \n",
    "                            shuffle=False, num_workers=0)\n",
    "    \n",
    "    export_features(model, full_loader, device, \"tabular_only_features.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"✓ BEST C-INDEX: {best_c_index:.4f}\")\n",
    "    print(\"⚠️  This uses the SAME patient cohort as all other benchmarks\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# \"\"\"\n",
    "# Benchmark 3: Tabular-Only Deep Neural Network\n",
    "# Tests whether deep learning on clinical features alone helps\n",
    "\n",
    "# Architecture:\n",
    "# - Deep feedforward network on tabular data\n",
    "# - LSTM for temporal modeling\n",
    "# - NO images\n",
    "# - Shows value of adding imaging modality\n",
    "# \"\"\"\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from lifelines.utils import concordance_index\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIG = {\n",
    "#     'tab_hidden': 256,\n",
    "#     'lstm_hidden': 128,\n",
    "#     'lstm_layers': 2,\n",
    "#     'dropout': 0.3,\n",
    "#     'lr': 5e-4,\n",
    "#     'weight_decay': 1e-4,\n",
    "#     'epochs': 100,\n",
    "#     'batch_size': 16,\n",
    "#     'max_seq_len': 10,\n",
    "#     'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# }\n",
    "\n",
    "# # ============================================================================\n",
    "# # FEATURE ENGINEERING (Same as thesis)\n",
    "# # ============================================================================\n",
    "\n",
    "# STATIC_FEATURES = [\n",
    "#     'age_bl', 'PTGENDER_encoded', 'PTEDUCAT', 'PTETHCAT_encoded', \n",
    "#     'PTRACCAT_encoded', 'PTMARRY_encoded'\n",
    "# ]\n",
    "\n",
    "# TEMPORAL_FEATURES = [\n",
    "#     'time_from_baseline', 'AGE', 'age_since_bl', 'mmse_slope', \n",
    "#     'adas13_slope', 'dx_progression', 'cog_decline_index', \n",
    "#     'visit_number', 'MMSE', 'ADAS13'\n",
    "# ]\n",
    "\n",
    "# def engineer_features(df):\n",
    "#     df = df.copy()\n",
    "    \n",
    "#     df[\"time_from_baseline\"] = df[\"Years_bl\"] - df[\"Years_bl\"].iloc[0]\n",
    "#     df[\"age_bl\"] = df[\"AGE\"].iloc[0]\n",
    "#     df[\"age_since_bl\"] = df[\"AGE\"] - df[\"age_bl\"]\n",
    "    \n",
    "#     df[\"mmse_slope\"] = df[\"MMSE\"].diff() / df[\"Years_bl\"].diff()\n",
    "#     df[\"adas13_slope\"] = df[\"ADAS13\"].diff() / df[\"Years_bl\"].diff()\n",
    "    \n",
    "#     dx_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2, \"Dementia\": 2}\n",
    "#     df[\"dx_progression\"] = df[\"DX\"].map(dx_map).diff()\n",
    "    \n",
    "#     df[\"cog_decline_index\"] = df[\"ADAS13\"] - df[\"MMSE\"]\n",
    "#     df[\"visit_number\"] = range(len(df))\n",
    "    \n",
    "#     df['age_mmse_interaction'] = df['AGE'] * (30 - df['MMSE']) / 30\n",
    "#     df['education_cognitive_reserve'] = df['PTEDUCAT'] * df['MMSE'] / 30\n",
    "#     df['rapid_decline_flag'] = (df['mmse_slope'] < -2).astype(float)\n",
    "    \n",
    "#     mmse_bins = [0, 20, 24, 30]\n",
    "#     df['mmse_severity'] = pd.cut(df['MMSE'], bins=mmse_bins, labels=[2, 1, 0]).astype(float)\n",
    "    \n",
    "#     df['weighted_mmse_decline'] = df['mmse_slope'] * np.exp(-0.1 * df['time_from_baseline'])\n",
    "#     df['mmse_variability'] = df['MMSE'].rolling(window=3, min_periods=1).std()\n",
    "    \n",
    "#     df['adas_mmse_discordance'] = np.abs(\n",
    "#         (df['ADAS13'] - df['ADAS13'].mean()) / (df['ADAS13'].std() + 1e-7) - \n",
    "#         (df['MMSE'] - df['MMSE'].mean()) / (df['MMSE'].std() + 1e-7)\n",
    "#     )\n",
    "    \n",
    "#     df = df.fillna(0)\n",
    "#     return df\n",
    "\n",
    "# TEMPORAL_FEATURES.extend([\n",
    "#     'age_mmse_interaction', 'education_cognitive_reserve', 'rapid_decline_flag',\n",
    "#     'mmse_severity', 'weighted_mmse_decline', 'mmse_variability', 'adas_mmse_discordance'\n",
    "# ])\n",
    "\n",
    "# # ============================================================================\n",
    "# # DATASET\n",
    "# # ============================================================================\n",
    "\n",
    "# class TabularOnlyDataset(Dataset):\n",
    "#     def __init__(self, manifest, max_seq_len=10):\n",
    "#         self.sequences = []\n",
    "#         self.max_seq_len = max_seq_len\n",
    "        \n",
    "#         manifest[\"path\"] = manifest[\"path\"].str.replace(\"\\\\\", \"/\", regex=False)\n",
    "#         manifest[\"path\"] = \"./AD_Multimodal/TFN_AD/\" + manifest[\"path\"]\n",
    "        \n",
    "#         for ptid in manifest['PTID'].unique():\n",
    "#             try:\n",
    "#                 patient_rows = manifest[manifest['PTID'] == ptid]\n",
    "#                 if len(patient_rows) == 0:\n",
    "#                     continue\n",
    "                \n",
    "#                 df = pd.read_pickle(patient_rows.iloc[0][\"path\"])\n",
    "#                 df = engineer_features(df)\n",
    "                \n",
    "#                 dx_seq = df[\"DX\"].tolist()\n",
    "#                 if \"MCI\" not in dx_seq:\n",
    "#                     continue\n",
    "                \n",
    "#                 mci_idx = dx_seq.index(\"MCI\")\n",
    "#                 ad_idx = next((i for i, x in enumerate(dx_seq[mci_idx+1:], start=mci_idx+1) \n",
    "#                               if x in [\"AD\", \"Dementia\"]), -1)\n",
    "                \n",
    "#                 if ad_idx != -1:\n",
    "#                     time_to_event = df[\"Years_bl\"].iloc[ad_idx]\n",
    "#                     event = 1\n",
    "#                 else:\n",
    "#                     time_to_event = df[\"Years_bl\"].iloc[-1]\n",
    "#                     event = 0\n",
    "                \n",
    "#                 tabs, times = [], []\n",
    "#                 valid_visits = 0\n",
    "                \n",
    "#                 for _, visit in df.iterrows():\n",
    "#                     tabs.append(visit[TEMPORAL_FEATURES + STATIC_FEATURES].values.astype(np.float32))\n",
    "#                     times.append(visit[\"Years_bl\"])\n",
    "#                     valid_visits += 1\n",
    "                    \n",
    "#                     if valid_visits >= max_seq_len:\n",
    "#                         break\n",
    "                \n",
    "#                 if valid_visits < 2:\n",
    "#                     continue\n",
    "                \n",
    "#                 pad_len = max_seq_len - len(tabs)\n",
    "#                 if pad_len > 0:\n",
    "#                     for _ in range(pad_len):\n",
    "#                         tabs.append(np.zeros_like(tabs[-1]))\n",
    "#                         times.append(times[-1])\n",
    "                \n",
    "#                 self.sequences.append({\n",
    "#                     'ptid': ptid,\n",
    "#                     'tabs': np.array(tabs, dtype=np.float32),\n",
    "#                     'times': np.array(times, dtype=np.float32),\n",
    "#                     'seq_len': valid_visits,\n",
    "#                     'time_to_event': time_to_event,\n",
    "#                     'event': event\n",
    "#                 })\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 continue\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         seq = self.sequences[idx]\n",
    "#         return (\n",
    "#             seq['tabs'], seq['times'], seq['seq_len'],\n",
    "#             seq['time_to_event'], seq['event'], seq['ptid']\n",
    "#         )\n",
    "\n",
    "# # ============================================================================\n",
    "# # MODEL\n",
    "# # ============================================================================\n",
    "\n",
    "# class TabularOnlyModel(nn.Module):\n",
    "#     def __init__(self, tab_dim, config):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "        \n",
    "#         # Deep tabular encoder\n",
    "#         self.tab_encoder = nn.Sequential(\n",
    "#             nn.Linear(tab_dim, config['tab_hidden']),\n",
    "#             nn.BatchNorm1d(config['tab_hidden']),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(config['dropout']),\n",
    "#             nn.Linear(config['tab_hidden'], config['tab_hidden'] // 2),\n",
    "#             nn.BatchNorm1d(config['tab_hidden'] // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(config['dropout']),\n",
    "#             nn.Linear(config['tab_hidden'] // 2, 128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "#         # LSTM for temporal modeling\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=128,\n",
    "#             hidden_size=config['lstm_hidden'],\n",
    "#             num_layers=config['lstm_layers'],\n",
    "#             batch_first=True,\n",
    "#             dropout=config['dropout'] if config['lstm_layers'] > 1 else 0,\n",
    "#             bidirectional=True\n",
    "#         )\n",
    "        \n",
    "#         # Risk prediction\n",
    "#         self.risk_head = nn.Sequential(\n",
    "#             nn.Linear(config['lstm_hidden'] * 2, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(config['dropout']),\n",
    "#             nn.Linear(64, 1)\n",
    "#         )\n",
    "    \n",
    "#     def encode_tabular(self, tab):\n",
    "#         return self.tab_encoder(tab)\n",
    "    \n",
    "#     def forward(self, tab_seq, seq_lengths):\n",
    "#         batch_size, seq_len, _ = tab_seq.shape\n",
    "        \n",
    "#         # Encode each timestep\n",
    "#         tab_feats = []\n",
    "#         for t in range(seq_len):\n",
    "#             feat = self.encode_tabular(tab_seq[:, t])\n",
    "#             tab_feats.append(feat)\n",
    "        \n",
    "#         tab_seq_feat = torch.stack(tab_feats, dim=1)\n",
    "        \n",
    "#         # LSTM\n",
    "#         packed = nn.utils.rnn.pack_padded_sequence(\n",
    "#             tab_seq_feat, seq_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "#         )\n",
    "#         lstm_out, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "#         h_forward = h_n[-2]\n",
    "#         h_backward = h_n[-1]\n",
    "#         h_final = torch.cat([h_forward, h_backward], dim=1)\n",
    "        \n",
    "#         risk_score = self.risk_head(h_final)\n",
    "        \n",
    "#         return risk_score\n",
    "\n",
    "# # ============================================================================\n",
    "# # LOSS & TRAINING\n",
    "# # ============================================================================\n",
    "\n",
    "# def cox_loss(risk_scores, times, events):\n",
    "#     order = torch.argsort(times, descending=True)\n",
    "#     risk_scores = risk_scores[order]\n",
    "#     events = events[order]\n",
    "    \n",
    "#     log_risk = risk_scores.view(-1)\n",
    "#     log_cumsum_hazard = torch.logcumsumexp(log_risk, dim=0)\n",
    "#     loss = -(log_risk - log_cumsum_hazard) * events\n",
    "    \n",
    "#     return loss.sum() / (events.sum() + 1e-7)\n",
    "\n",
    "# def train_epoch(model, loader, optimizer, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for batch in loader:\n",
    "#         tabs, times, seq_lens, t_event, event, _ = batch\n",
    "        \n",
    "#         tabs = torch.FloatTensor(tabs).to(device)\n",
    "#         seq_lens = torch.LongTensor(seq_lens)\n",
    "#         t_event = t_event.float().to(device)\n",
    "#         event = event.float().to(device)\n",
    "        \n",
    "#         risk_scores = model(tabs, seq_lens)\n",
    "#         loss = cox_loss(risk_scores.squeeze(), t_event, event)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     return total_loss / len(loader)\n",
    "\n",
    "# def validate(model, loader, device):\n",
    "#     model.eval()\n",
    "#     all_risks, all_times, all_events = [], [], []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             tabs, times, seq_lens, t_event, event, _ = batch\n",
    "            \n",
    "#             tabs = torch.FloatTensor(tabs).to(device)\n",
    "#             seq_lens = torch.LongTensor(seq_lens)\n",
    "            \n",
    "#             risk_scores = model(tabs, seq_lens)\n",
    "            \n",
    "#             all_risks.extend(risk_scores.cpu().numpy().flatten())\n",
    "#             all_times.extend(t_event.numpy())\n",
    "#             all_events.extend(event.numpy())\n",
    "    \n",
    "#     all_events = np.array(all_events).astype(bool)\n",
    "#     all_times = np.array(all_times)\n",
    "#     all_risks = np.array(all_risks)\n",
    "    \n",
    "#     c_index = concordance_index(all_times, -all_risks, all_events)\n",
    "#     return c_index\n",
    "\n",
    "# # ============================================================================\n",
    "# # EXPORT\n",
    "# # ============================================================================\n",
    "\n",
    "# def export_features(model, loader, device, output_path):\n",
    "#     \"\"\"Export tabular features - SURVIVAL DATA ONLY\"\"\"\n",
    "#     model.eval()\n",
    "#     rows = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             tabs, times, seq_lens, t_event, event, ptids = batch\n",
    "            \n",
    "#             tabs = torch.FloatTensor(tabs).to(device)\n",
    "#             seq_lens = torch.LongTensor(seq_lens)\n",
    "            \n",
    "#             # Only use FIRST visit for each patient (baseline)\n",
    "#             for i in range(len(ptids)):\n",
    "#                 tab_feat = model.encode_tabular(tabs[i:i+1, 0])\n",
    "#                 tab_vals = tabs[i, 0].cpu().numpy()\n",
    "                \n",
    "#                 row = {\n",
    "#                     \"PTID\": ptids[i],\n",
    "#                     \"Years_bl\": float(times[i][0]),\n",
    "#                     \"time_to_event\": float(t_event[i]),\n",
    "#                     \"event\": int(event[i])\n",
    "#                 }\n",
    "                \n",
    "#                 feat_vals = tab_feat[0].cpu().numpy()\n",
    "#                 for k in range(len(feat_vals)):\n",
    "#                     row[f\"tab_feat_{k}\"] = float(feat_vals[k])\n",
    "                \n",
    "#                 rows.append(row)\n",
    "    \n",
    "#     df = pd.DataFrame(rows).sort_values(['PTID', 'Years_bl'])\n",
    "#     df.to_csv(output_path, index=False)\n",
    "    \n",
    "#     print(f\"\\n✓ Exported to {output_path}\")\n",
    "#     print(f\"  Patients: {df['PTID'].nunique()}, Features: {len([c for c in df.columns if c.startswith('tab_feat')])}\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # ============================================================================\n",
    "# # MAIN\n",
    "# # ============================================================================\n",
    "\n",
    "# def main():\n",
    "#     print(\"=\" * 80)\n",
    "#     print(\"BENCHMARK 3: TABULAR-ONLY DEEP NETWORK\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     device = CONFIG['device']\n",
    "#     print(f\"\\nDevice: {device}\")\n",
    "    \n",
    "#     manifest = pd.read_csv(\"./AD_Multimodal/TFN_AD/AD_Patient_Manifest.csv\")\n",
    "    \n",
    "#     dataset = TabularOnlyDataset(manifest, max_seq_len=CONFIG['max_seq_len'])\n",
    "#     print(f\"Total sequences: {len(dataset)}\")\n",
    "    \n",
    "#     n_train = int(0.8 * len(dataset))\n",
    "#     n_val = len(dataset) - n_train\n",
    "#     train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "#         dataset, [n_train, n_val], generator=torch.Generator().manual_seed(42)\n",
    "#     )\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "#                              shuffle=True, num_workers=0)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
    "#                            shuffle=False, num_workers=0)\n",
    "    \n",
    "#     sample_batch = next(iter(train_loader))\n",
    "#     tab_dim = sample_batch[0].shape[2]\n",
    "#     print(f\"Tabular features: {tab_dim}\")\n",
    "    \n",
    "#     model = TabularOnlyModel(tab_dim, CONFIG).to(device)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'], \n",
    "#                                   weight_decay=CONFIG['weight_decay'])\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='max', factor=0.5, patience=5\n",
    "#     )\n",
    "    \n",
    "#     print(\"\\nTraining...\")\n",
    "#     best_c_index = 0\n",
    "#     patience_counter = 0\n",
    "    \n",
    "#     for epoch in range(CONFIG['epochs']):\n",
    "#         train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "#         val_c_index = validate(model, val_loader, device)\n",
    "        \n",
    "#         scheduler.step(val_c_index)\n",
    "        \n",
    "#         print(f\"Epoch {epoch+1}/{CONFIG['epochs']} - Loss: {train_loss:.4f}, C-index: {val_c_index:.4f}\")\n",
    "        \n",
    "#         if val_c_index > best_c_index:\n",
    "#             best_c_index = val_c_index\n",
    "#             torch.save(model.state_dict(), 'tabular_only_model.pth')\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "        \n",
    "#         if patience_counter >= 15:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "    \n",
    "#     # Export\n",
    "#     model.load_state_dict(torch.load('tabular_only_model.pth'))\n",
    "#     full_loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], \n",
    "#                             shuffle=False, num_workers=0)\n",
    "    \n",
    "#     export_features(model, full_loader, device, \"tabular_only_features.csv\")\n",
    "    \n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(f\"✓ BEST C-INDEX: {best_c_index:.4f}\")\n",
    "#     print(\"=\" * 80)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "    \n",
    "\n",
    "# # \"\"\"\n",
    "# # STEP 2: BASELINE METHOD - Export Clinical Features Only\n",
    "# # This creates the comparison baseline WITHOUT tensor fusion/autoencoder\n",
    "# # Same 161 patients, same preprocessing, just no deep learning\n",
    "# # \"\"\"\n",
    "\n",
    "# # import pandas as pd\n",
    "# # import numpy as np\n",
    "# # import os\n",
    "\n",
    "# # def engineer_features(df):\n",
    "# #     \"\"\"Same feature engineering as thesis method\"\"\"\n",
    "# #     df = df.copy()\n",
    "# #     df[\"time_from_baseline\"] = df[\"Years_bl\"] - df[\"Years_bl\"].iloc[0]\n",
    "# #     df[\"age_bl\"] = df[\"AGE\"].iloc[0]\n",
    "# #     df[\"age_since_bl\"] = df[\"AGE\"] - df[\"age_bl\"]\n",
    "# #     df[\"mmse_slope\"] = df[\"MMSE\"].diff() / df[\"Years_bl\"].diff()\n",
    "# #     df[\"adas13_slope\"] = df[\"ADAS13\"].diff() / df[\"Years_bl\"].diff()\n",
    "# #     dx_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2, \"Dementia\": 2}\n",
    "# #     df[\"dx_progression\"] = df[\"DX\"].map(dx_map).diff()\n",
    "# #     df[\"cog_decline_index\"] = df[\"ADAS13\"] - df[\"MMSE\"]\n",
    "# #     df[\"visit_number\"] = range(len(df))\n",
    "# #     df = df.fillna(0)\n",
    "# #     return df\n",
    "\n",
    "# # print(\"=\"*80)\n",
    "# # print(\"BASELINE METHOD: Exporting Clinical Features\")\n",
    "# # print(\"=\"*80)\n",
    "\n",
    "# # # Load manifest\n",
    "# # manifest = pd.read_csv(\"./AD_Multimodal/TFN_AD/AD_Patient_Manifest.csv\")\n",
    "# # manifest[\"path\"] = manifest[\"path\"].str.replace(\"\\\\\", \"/\", regex=False)\n",
    "# # manifest[\"path\"] = \"./AD_Multimodal/TFN_AD/\" + manifest[\"path\"]\n",
    "\n",
    "# # rows = []\n",
    "# # patients_processed = 0\n",
    "\n",
    "# # for idx, row in manifest.iterrows():\n",
    "# #     if not os.path.exists(row[\"path\"]):\n",
    "# #         continue\n",
    "    \n",
    "# #     try:\n",
    "# #         df = pd.read_pickle(row[\"path\"])\n",
    "# #         df = engineer_features(df)\n",
    "# #         dx_seq = df[\"DX\"].tolist()\n",
    "        \n",
    "# #         # Only MCI patients\n",
    "# #         if \"MCI\" not in dx_seq:\n",
    "# #             continue\n",
    "        \n",
    "# #         # Compute survival outcome\n",
    "# #         mci_idx = dx_seq.index(\"MCI\")\n",
    "# #         ad_idx = next((i for i, x in enumerate(dx_seq[mci_idx+1:], start=mci_idx+1)\n",
    "# #                        if x in [\"AD\", \"Dementia\"]), -1)\n",
    "        \n",
    "# #         if ad_idx != -1:\n",
    "# #             time_to_event = df[\"Years_bl\"].iloc[ad_idx]\n",
    "# #             event = 1\n",
    "# #         else:\n",
    "# #             time_to_event = df[\"Years_bl\"].iloc[-1]\n",
    "# #             event = 0\n",
    "        \n",
    "# #         # Export each visit with ONLY clinical/tabular features\n",
    "# #         for _, visit in df.iterrows():\n",
    "# #             # Check if image exists (for consistency with thesis method)\n",
    "# #             image_path = visit[\"image_path\"].replace(\n",
    "# #                 \"/home/mason/ADNI_Dataset/\", \n",
    "# #                 \"./AD_Multimodal/ADNI_Dataset/\"\n",
    "# #             )\n",
    "            \n",
    "# #             if not os.path.exists(image_path):\n",
    "# #                 continue\n",
    "            \n",
    "# #             # Extract clinical features\n",
    "# #             row_data = {\n",
    "# #                 \"PTID\": visit[\"PTID\"],\n",
    "# #                 \"Years_bl\": visit[\"Years_bl\"],\n",
    "# #                 \"time_to_event\": time_to_event,\n",
    "# #                 \"event\": event,\n",
    "                \n",
    "# #                 # Core clinical features (like benchmark)\n",
    "# #                 \"MMSE\": visit[\"MMSE\"],\n",
    "# #                 \"ADAS13\": visit[\"ADAS13\"],\n",
    "# #                 \"AGE\": visit[\"AGE\"],\n",
    "# #                 \"PTGENDER\": visit[\"PTGENDER_encoded\"],\n",
    "# #                 \"PTEDUCAT\": visit[\"PTEDUCAT\"],\n",
    "                \n",
    "# #                 # Additional demographics\n",
    "# #                 \"PTETHCAT\": visit[\"PTETHCAT_encoded\"],\n",
    "# #                 \"PTRACCAT\": visit[\"PTRACCAT_encoded\"],\n",
    "# #                 \"PTMARRY\": visit[\"PTMARRY_encoded\"],\n",
    "                \n",
    "# #                 # Engineered features (your contribution)\n",
    "# #                 \"age_bl\": visit[\"age_bl\"],\n",
    "# #                 \"age_since_bl\": visit[\"age_since_bl\"],\n",
    "# #                 \"mmse_slope\": visit[\"mmse_slope\"],\n",
    "# #                 \"adas13_slope\": visit[\"adas13_slope\"],\n",
    "# #                 \"dx_progression\": visit[\"dx_progression\"],\n",
    "# #                 \"cog_decline_index\": visit[\"cog_decline_index\"],\n",
    "# #                 \"visit_number\": visit[\"visit_number\"],\n",
    "# #             }\n",
    "            \n",
    "# #             rows.append(row_data)\n",
    "        \n",
    "# #         patients_processed += 1\n",
    "        \n",
    "# #     except Exception as e:\n",
    "# #         print(f\"⚠️ Error processing {row['path']}: {e}\")\n",
    "# #         continue\n",
    "\n",
    "# # # Create DataFrame\n",
    "# # baseline_df = pd.DataFrame(rows)\n",
    "\n",
    "# # print(f\"\\n✓ Processed {patients_processed} patients\")\n",
    "# # print(f\"✓ Total visits: {len(baseline_df)}\")\n",
    "# # print(f\"✓ Events: {baseline_df['event'].sum()}\")\n",
    "# # print(f\"✓ Event rate: {baseline_df.groupby('PTID')['event'].first().mean()*100:.1f}%\")\n",
    "\n",
    "# # # Save\n",
    "# # baseline_df.to_csv(\"baseline_clinical_features.csv\", index=False)\n",
    "# # print(f\"\\n✓ Saved to: baseline_clinical_features.csv\")\n",
    "\n",
    "# # print(\"\\n\" + \"=\"*80)\n",
    "# # print(\"BASELINE FEATURES EXPORTED\")\n",
    "# # print(\"=\"*80)\n",
    "# # print(\"This dataset contains ONLY clinical/tabular features.\")\n",
    "# # print(\"Use this for comparison against your thesis method (fusion + autoencoder).\")\n",
    "# # print(\"\\nFeatures included:\")\n",
    "# # print(\"  - Core clinical: MMSE, ADAS13, AGE, PTGENDER, PTEDUCAT\")\n",
    "# # print(\"  - Demographics: PTETHCAT, PTRACCAT, PTMARRY\")\n",
    "# # print(\"  - Engineered: slopes, progression, decline index\")\n",
    "# # print(f\"\\nNext: Run your thesis method (fusion + autoencoder) on same {patients_processed} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724aed52-d50c-4638-9e0c-c4b842c35872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
